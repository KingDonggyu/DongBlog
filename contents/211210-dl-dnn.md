---
date: "2021-12-10"
title: "[Deep Learning] DNN 기본 코드 이해"
category: "Data Science"
categoryColor: "seagreen"
tags: ["AI", "DL"]
thumbnail: "./images/DL.jpeg"
---

# 전체구조 개요

<hr />

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tensorflow as tf

tf.random.set_seed(3)

model = Sequential()
model.add(Dense(60, input_dim=17, activation='relu'))
model.add(Dense(60, activation='relu'))
model.add(Dense(60, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 딥러닝 실행
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(x, y, epochs=30, batch_size=10)
```

- `Sequential()` : 딥러닝의 구조를 짜고 층을 설정하는 부분

<div>
  <img src="https://user-images.githubusercontent.com/33220404/145545830-b9aee4b0-3027-42a5-928f-42a6a6f07a1a.png" width="400">
</div>

- `model.compile()` : 위에서 정해진 모델을 컴퓨터가 알아들을 수 있게끔 컴파일하는 부분

- `model.fit()` : 모델을 실제로 수행하는 부분

<br />

## 1. 모델 레이어

- 모델 내부

  - `model.add()`라는 라인을 추가하면 새로운 층이 만들어진다.

  - 첫 레이어만 `input_dim` 설정이 필요하고 나머지는 알아서 설정된다.

- 은닉층/출력층

  - 맨 마지막 층은 결과를 출력하는 **‘출력층’** 이 되며, 나머지는 모두 **‘은닉층’** 의 역할을 한다.

  ```python
  # hidden layer - 은닉층
  model.add(Dense(60, input_dim=17, activation='relu'))
  model.add(Dense(60, activation='relu'))
  model.add(Dense(60, activation='relu'))

  # output layer - 출력층
  model.add(Dense(1, activation='sigmoid'))
  ```

  - 출력층은 왜 노드가 1개일까?

    - **binary decision classifier**로 출력 값을 하나로 정해서 보여 줘야 하므로 출력 층의 노드 수는 1개이다. (노드를 여러개로도 가능하다.)

  - 출력층은 왜 **sigmoid**일까?

    - 계단함수는 딱딱한 의사결정을 한다. 그에 비해 **Logistic sigmoid**는 부드러운 의사결정이 가능하다.

<div style="text-align:center;">
  <img src="https://user-images.githubusercontent.com/33220404/145546874-8dd357c3-4597-4b08-b3a8-e6524302c627.png" width="350">
</div>

<br />

## 2. 컴파일

- `model.compile()` : 앞서 지정한 모델이 효과적으로 구현될 수 있게 여러 가지 **환경을 설정**해주면서 컴파일하는 부분

  ```python
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  ```

  - 오차 함수 : MSE계열과 cross-entropy 계열의 함수가 있다.

    - **MSE계열**의 함수는 수렴하기까지 **속도가 많이 걸린다**는 단점이 있다. (오차가 작을 경우에 Regression에 유리하다.)

      ```python
      model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
      ```

  - `metrics()` 함수 : 모델이 컴파일될 때 **모델 수행 결과**를 나타내게끔 설정하는 부분

    - 테스트 샘플을 학습 과정에서 제외시킨다. ➡️ 과적합 문제를 방지하는 기능을 담당한다.

  - `Optimizer`

    - 어떻게/얼마나 빠르게 최적화된 gradient들을 찾을 것인가에 대한 문제.

    - 보통 adam부터 시작하면 좋은 편이다.

    - SGD, RMSprop, Nadam 등 다양한 기법이 존재한다. (adam이 비교적 빠르고 정확한 편)

<br />

## 3. 모델 실행

- `model.fit()` : 주어진 데이터(X, Y)를 불러 실행시킬 때 사용되는 함수

  ```python
  model.fit(x, y, epochs=30, batch_size=10)
  ```

  - **1 epoch** : 학습 프로세스가 모든 샘플에 대해 한번 실행되는 것

    - `epochs = 100` : 각 샘플이 처음부터 끝까지 100번 재사용될 때까지 실행을 반복하라는 뜻

    - **gradient decent** 기반이므로 반복횟수가 중요하다.

  - `batch_size` : 샘플을 한 번에 몇 개씩 처리할지를 정하는 부분

    - `batch_size = 10`은 전체 470개의 샘플을 10개씩 끊어서 집어넣으라는 뜻

    - `batch_size`가 너무 **크면 학습 속도가 느려지고, 너무 작으면 각 실행 값의 편차가 생겨서 전체 결과값이 불안정해질 수 있다.**

      (속도로 인해 성능보다 `batch_size`가 중요하다)

    - 메모리(RAM, GPU)를 고려하여 설정해야 한다.

<br />

## 4. 데이터 표현/전처리 : MNIST dataset

- MNIST 데이터의 shape

  ```python
  from keras.datasets import mnist
  (train_images, train_labels), (test_images, test_labels) = mnist.load_data()

  # >>> train)images.shape
  # (60000, 28, 28)  - 가로/세로 28, 60000개의 샘플이 있다.
  # >>> test_images.shape
  # (10000, 28, 28)
  ```

- Input_shape

  ```python
  from keras import models
  from keras import layers

  network = models.Sequential()
  network.add(layer.Dense(512, activation='relu', input_shape=(28 * 28)))
  network.add(layer.Dense(10, activation='softmax'))
  ```

<br />

## 4. 데이터 표현/전처리 : 텐서(Tensor)

**텐서 실제사례** - 우리가 사용할 데이터는 대부분 다음 중 하나에 속할 것이다.

- **벡터 데이터** : (samples, features) ➡️ **2D** 텐서

- **시계열 데이터**/**시퀀스(sequence) 데이터** : (samples, timesteps, features) ➡️ **3D** 텐서

  - property 뿐만 아니라 시간 개념이 추가된다.

<div style="text-align:center;">
  <img src="https://user-images.githubusercontent.com/33220404/145552510-225bedca-83ff-4498-875b-7e7afd29e068.png" width="350">
</div>

- **이미지** : (samples, height, width, channels) 또는 (samples, channels, height, width) ➡️ **4D** 텐서

<div style="text-align:center;">
  <img src="https://user-images.githubusercontent.com/33220404/145553242-5b4cda65-96f3-41a6-b7d1-5484013929be.png" width="300">
</div>

- **동영상** : (samples, frames, height, width, channels) 또는 (samples, frames, channels, height, width) ➡️ **5D** 텐서

<br />

**텐서 크기 변환**

- 보통 신경망에 주입할 숫자 **데이터를 전처리할 때** 사용한다.

- 아래 신경망 예제의 Dense 층에 1D `input_shape` 으로 넣기 위해서도 사용된다.

  ```python
  from keras import models
  from keras import layers

  network = models.Sequential()
  network.add(layer.Dense(512, activation='relu', input_shape=(28 * 28)))
  network.add(layer.Dense(10, activation='softmax'))
  ```

- ex) **3D** Tensor ➡️ **2D** Tensor

<div>
  <img src="https://user-images.githubusercontent.com/33220404/145554156-1311c2f0-ca80-4e7d-81c4-d3ce51dfb6e4.png" width="700">
</div>

<br />

# DNN 다중분류문제 & DNN 회귀문제

<hr />

## DNN 다중분류문제

### one-hot encoding

여러 개의 Y 값을 0과 1로만 이루어진 형태를 바꿔주는 기법으로, 다중분류의 계산측면에서 좀 더 유리할 수 있다.

```python
Y_encoded = tf.keras.utils.to_categorical(Y)
# array([1, 2, 3])가 array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])로 바뀐다.
```

### 모델

```python
# 모델의 설정
model = Sequential()
model.add(Dense(16, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))  # 출력층 노드 3개

# 모델 컴파일
model.compile(loss = 'categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'])
```

- 출력층 노드가 2개 이상이 되므로, 다중분류에 유효한 `categorical_crossentropy`를 사용한다.

- **소프트맥스**: 총합이 1인 형태로 바꿔서 계산해 주는 함수이다.

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145556350-9579681c-5245-48f2-9f89-9feed3de318c.png" width="500">
</div>

<br />

## DNN 회귀문제

### 모델

**회귀**는 마지막에 참과 거짓을 구분할 필요가 없으므로, **출력층에 활성화 함수를 지정할 필요도 없다.**

(이럴 경우 단순 weighted sum이 출력됨)

```python
model = Sequential()
model.add(Dense(30, input_dim=13, activation='relu'))
model.add(Dense(6, activation='relu')) 
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, Y_train, epochs=200, batch_size=10)
```
