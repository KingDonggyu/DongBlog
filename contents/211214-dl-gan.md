---
date: "2021-12-14"
title: "[Deep Learning] GAN"
category: "Data Science"
categoryColor: "seagreen"
tags: ["AI", "DL"]
thumbnail: "./images/DL.jpeg"
---

> Generative Adversarial Network (생성적 적대 신경망)은 신경망을 서로 겨루게 하고 경쟁을 통해 신경망을 향상하는 것을 기대한다. GAN은 다른 목표를 가진 두 네트워크로 구성되므로, 훈련 반복이 Generator 훈련, Discriminator 훈련 두 단계로 이루어 진다.

**생성자**는 AE의 디코더와 비슷한 모습이다. **판별자**는 일반적인 이진 분류기이다.

## Generator (생성자, like 가짜 제조 공장)

가상의 이미지를 만들어 내는 공장과 같다.

처음엔 랜덤한 픽셀 값으로 채워진 가짜 이미지로 시작하여, 판별자의 판별 결과에 따라 지속적으로 업데이트한다. 이렇게 점차 원하는 이미지를 만들어간다.

## Discriminator (판별자, liek 진위 판별 장치)

생성자에게 넘어온 이미지가 가짜인지 진짜인지를 판별해 주는 장치와 같다.

진짜(1) 아니면 가짜(0), 둘 중 하나를 결정하는 문제이다.

많이 쓰던 CNN 구조를 그대로 가지고 와도 된다. 컨볼루션 신경망이란게 원래 무언가를 (예를 들면 개와 고양이 사진을) **구별하는 데에 최적화된 알고리즘이기 때문에** 그 목적 그대로 사용하면 된다.

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145706634-0ea24f20-2899-4389-8866-d2a7b745e1cf.png" width="500">
</div>

위 이미지는 **생성자 모델이 G(), 판별자 모델이 D(), 실제 데이터가 x, 입력 값이 input일 때의 GAN의 기본 구조**이다.

<br />

## GAN: how to train

**Training with two phases.**

- **First: 판별자를 훈련한다.**

  - **1. 실제 이미지(label:1)인 훈련 세트에서 batch를 샘플링한다.**

  - **2. 생성자(label:0)에서 생성된 가짜 이미지에서 동일한 수의 batch를 샘플링한다.**

  (실제 이미지와 가짜 이미지는 동일하게 생플링된다.)

  - 여기에서만 BP를 사용하면 판별자를 훈련한다.

- **Second: 생성자를 훈련한다.**

  - **1. 생성자에서 생성한 가짜 이미지에서 대른 샘플 batch를 만든다.**

    - 이 batch에서는 실제 이미지를 추가하지 않지만 모든 label은 1(real)로 설정된다.

  - **2. 다시 판별자는 이미지가 가짜인지 진짜인지 구별하는 데 사용된다.**

    ➡️ 판별자가 (wrongly) 실제라고 믿는 이미지를 생성자가 생성하도록 한다.

  (이 단계에서 판별자의 가중치가 고정된다.)

  - BP는 판별자가 아닌 생성자의 가중치에만 영향을 준다.

**=> 생성자는 실제 이미지를 실제로 보지는 않지만 가짜 이미지를 생성하는 방법을 배운다.**

<br />

## The Difficulties of Training GANs

훈련 과정에서 생성자와 판별자는 끊임없이 서로 앞서려고 한다 (**➡️ zero-sum game**).


### Mode Collapse

> 생성자의 출력이 점차적으로 **덜 다양해지는 것**을 말한다. 

생성자가 다른 클래스보다 이미지를 더 그럴듯하게 만든다고 가정한다.

- 이미지가 판별자를 속이기 쉽기 때문에, 더 많은 이미지를 만들도록 유도된다.

- 생성자는 점차 다른 이미지를 생성하는 방법을 잊게되고, 그동안 판별자가 보게 될 가짜 이미지는, 해당 이미지가 유일하게 된다.

- 따라서 판별자도 다른 클래스의 가짜 이미지를 구별하는 방법을 잊어버린다.

- 판별자가 이미지를 잘 구별하게 되면, 생성자는 다른 클래스로 옮겨가야 한다.

- 옮겨가서 잘 훈련을 하게 되면, 기존 이미지에 대해서는 또 잊어버리게 되고, 그렇게 되면 판별자도 뒤따라 가게 된다.

**➡️ 결론적으로, 이 GAN은 몇 개의 클래스에 걸쳐 점진적으로 순환하다가, 그 중 어떤 클래스도 실제로 좋은 결과를 만들지 못할 수 있다.**

<br />

### 파라미터의 변동

> 생성자와 판별자가 지속적으로 서로에게 영향을 주어, 파라미터 변동이 크고 불안정해질 수 있다. (훈련이 안정적으로 시작되어도, 이유 없이 갑자기 발산할 수 있다.)

**➡️ 여러 요인이 이런 복잡한 관계에 영향을 주어 GAN의 하이퍼파라미터는 매우 민감하며, 이런 하이퍼파라미터 튜닝을 위해서 많은 노력이 필요하다.**

<br />

## One Solution - mini-batch discrimination(판별)

batch 간에 얼마나 비슷한 이미지가 있는지 측정하여, 이 통계를 판별자에게 제공한다.

판별자는 **다양성이 부족한 가짜 이미지 batch 전체를 쉽게 거부**할 수 있다.

**➡️ 생성자가 다양한 이미지를 생성하도록 유도해, mode collapse의 위험을 줄인다.**

<br />

GAN은 여전히 활발히 연구 중이며, GAN의 역학은 아직 완벽하게 파악하지 못했다. 하지만 많은 진전이 있었고, 일부 모델은 매우 뛰어난 성능을 보여주었다.

<br />

# Existing GAN models

<hr />

> 2014년 GAN은 컨볼루션 층을 통해 작은 이미지만 생성했다. 그 후, 큰 이미지를 위해 깊은 컨볼루션 층 기반의 GAN을 만들기 위해 노력했고, 2015년, 여러 구조와 하이퍼파라미터 실험 끝애 Deep Convolutional GANs (DCGAN)이 제안되었다.

## 안정적인 컨볼루션 GAN을 구축하기 위한 가이드라인

- 판별자의 풀링 층을 **스트라이드 컨볼루션**으로 변경한다.

- 생성자의 풀링 층은 **transposed(전치) 컨볼루션**으로 변경한다.

- 생성자와 판별자에 **Batch Normalization**을 사용한다.

- 깊은 층을 위해 **fully connected hidden layers를 제거**한다.

- 생성자의 모든 층은 **ReLU 활성화 함수를 사용**한다.

- 판별자의 모든층은 **leaky ReLU 활성와 함수를 사용**한다.

(항상 맞는 것은 아니므로, 여러 실험이 필욜하다.)

<br />
<br />

**Source:**

📖 핸즈온머신러닝, 2/E, 2020 (번역)

📖 기계학습, 오일석, 2017

📖 모두의 딥러닝, 2/E, 2020