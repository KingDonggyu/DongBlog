---
date: "2021-12-13"
title: "[Deep Learning] RNN"
category: "Data Science"
categoryColor: "seagreen"
tags: ["AI", "DL"]
thumbnail: "./images/DL.jpeg"
---

> 순환 신경망 RNN은 일반적으로 고정 길이 입력이 아닌 임의 길이를 가진 시퀀스를 다룰 수 있다. 문장, 문서, 오디오 샘플을 입력으로 받을 수 있고, 자연어 처리에 매우 유용하다.

시계열 데이터를 분석해서 주식 가격 등을 예측하고, 자율 주행 시스템에서는 차의 이동 경로를 예측하여 사고를 피하도록 돕는다.

지금까지는 활성화 신호가 입력층에서 출력층 한 방향으로 흐르는 feedforward 신경망 위주였지만, **순환 신경망은 뒤쪽으로 순환하는 역방향 연결도 있다는 차이점이 있다.**

# Recurrent Neuron

<hr />

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145701830-b03e349c-1ea3-4532-b439-253103fd1c7b.png" width="600">
</div>

- 위 이미지는 입력을 받아 출력을 만들고, 자신에게도 출력을 보내는 뉴런 하나로 구성된 가장 간단한 RNN 구조이다. 오른쪽은 시간에 따라 네트워크를 펼쳐 표현한 것이다(동일한 뉴런을 타임 스텝마다 하나씩 표현).

- 각 **time step t**(or frame)마다 이 **순환 뉴런**은 물론 x(t)와 이전 time step의 출력인 y(t-1)을 입력으로 받는다.

- 첫 번째 타입스텝에서는 이전 출력이 없으므로 일반적으로 0으로 설정한다.

<br />

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145701962-33e009af-5724-4284-8623-4482e25c1456.png" width="600">
</div>

- 위 이미지는 순환 뉴런으로 된 층이다.

- 타임 스텝 t마다 모든 뉴런은 입력 벡터 x(t)와 이전 타임 스텝의 출력 벡터 y(t-1)을 받는다. (입력과 출력이 모두 벡터)

- 각 순환 뉴런은 **두 가중치 행렬**을 가진다.

  - 하나는 입력 x(i)를 위한 것(가중치 벡터 wx)이며, 하나는 이전 타임 스텝의 출력 y(t-1)를 위한 것(가중치 벡터 wy)이다.

<br />

## Recurrent Neural Network (RNN)

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145702051-bb98a7d5-f966-4fdd-8c81-3441964c6ea0.png" width="700">
</div>

MLP와 비슷하게 입력층, 은닉층, 출력층을 가진다.

순환 edge는 t-1 순간에 발생한 정보를 t 순간으로 전달하는 역할을 한다.

<br />

## Memory Cells

> 타입 스텝 t에서 순환 뉴런의 출력은 이전 타입 스텝의 모든 입력에 대한 함수이므로 일종의 메모리 형태라고 할 수 있다. 타입 스텝에 걸쳐서 어떤 상태를 보존하는 신경망의 구성 요소를 Memory Cell이라고 한다.

이는 하나의 순환 뉴런 또는 순환 뉴런의 층은 **짧은 패턴만 학습**할 수 있는 기본적인 셀이다.

일반적으로 타입 스텝 t에서의 셀의 상태 h(t)는 그 타임 스텝의 입력과 이전 타임 스텝의 상태에 대한 함수이다. (ht(t) = f(h(t-1), x(t)))

<br />

## RNN types

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145702182-d95eb500-5442-477d-9049-b7a130dd83a1.png" width="600">
</div>

- TL: **Sequence-to-sequence** : RNN은 입력 시퀀스를 받아 출력 시퀀스를 만들 수 있다.

  ex) 최근 N일치 주식가격을 주입하면 네트워크는 각 입력값보다 하루 앞선 가격을 출력해야 함.

- TR: **Sequence-to-vector** : 입력 시퀀스를 네트워크에 주입하고, 마지막을 제외한 모든 출력을 무시할 수 있다.

  ex) 영화 리뷰에 있는 연속된 단어를 주입하면 네트워크는 감성 점수를 출력한다.

- BL: **Vector-to-sequence** : 각 타임 스텝에서 하나의 입력 벡터를 반복해서 네트워크에 주입하고, 하나의 시퀀스를 출력할 수 있다.

  ex) 이미지(or CNN의 출력)를 입력하여 이미지에 대한 캡션을 출력할 수 있다.

- BR: **Encoder-Decoder** : 인코더라 부르는 **Sequence-to-vector** 뒤에 디코더라 부르는 **Vector-to-sequence** 네트워크를 연결할 수 있다.

  ex) 한 언어 문장을 다른 언어로 번역하는 데 사용할 수 있다. 한 언어의 문장을 네트워크에 주입하면, 인코더는 이 문장을 하나의 벡터로 표현하고, 그 이후 디코더가 이 벡터를 다른 언어의 문장으로 디코딩한다.

  - 인코더-디코더라 불리는 이중 단계 모델은 시퀀스-투-시퀀스가 RNN을 이용해 한 단어씩 번역하는 것보다 훨씬 더 잘 작동한다.

    - 문장의 마지막 단어가 번역의 첫번째 단어에 영향을 줄 수 있기 때문이다.

    - 번역하기 전에 전체 문장이 주입될 때까지 기다릴 필요가 있다.

<br />

# Training RNNs

<hr />

> RNN을 훈련하는 방법은, 타입 스텝으로 네트워크를 펼치고 보통의 역전파를 사용하는 것이다. ➡️ BPTT(Backpropagation Through Time)

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145702468-ddd6a77f-c82f-433b-8762-b830a9372f34.png" width="400">
</div>

1. forward pass on unrolled net (dashed arrows)

2. evaluate output sequence using cost function C()

3. Propagate gradients backward (solid arrows)

4. Update model parameters

- 보통의 역전파와 같이, 첫번째 정방향 패스가 펼쳐진 네트워크를 통과한다.

- 비용함수 C(Y(0), Y(1), Y(2))를 사용하여 출력 시퀀스가 평가된다.

- 비용함수의 그레디언트는 펼쳐진 네트워크를 따라 역방향으로 전파된다.

- 결국 모델의 파라미터는 BPTT 동안 계산된 그레디언트를 사용해 업데이트된다.

- 그레디언트가 마지막 출력 뿐만 아니라 **비용함수를 사용한 모든 출력에서 역방향으로 전파된다.**

  - 위 이미지에서는 비용함수 계산시 Y(2), Y(3), Y(4)를 사용했기 때문에 그레디언트는 이 세개의 출력을 거쳐 흐르지만 Y(0)과 Y(1)은 거치치 않는다.

- **각 타입 스텝마다 같은 매개변수 w와 b가 사용**되므로 역전파가 진행되면 모든 타임 스텝에 걸쳐 합산된다.

<br />

## Deep RNNs

RNN은 셀을 여러 층으로 쌓는 것이 일반적이다. 이를 **Deep(심층) RNN**이라 한다.

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145702662-3f30be5e-7220-471c-a0b0-0bc751957e42.png" width="400">
</div>

<br />

# Problem of RNN

<hr />

## Problem in handling LONG sequences

> 긴 시퀀스로 RNN을 훈련하려면, 많은 타임 스텝에 걸쳐 실행해야 하므로 펼친 RNN이 매우 깊은 네트워크가 된다.

1. 그레디언트 소실(w 요소가 1보다 작을 때) 또는 폭발(w 요소가 1보다 클 때) 문제가 있을 수 있다.

- RNN은 **긴 입력 샘플이 자주 발생**하기 때문에 DMLP나 CNN보다 더 심각하다. **가중치 공유** 때문에 같은 값을 계속 곱한다.

2. RNN이 긴 시퀀스를 처리할 때 입력의 첫 부분을 조금씩 잊어버리게 된다.

<br />

불안정한 그레디언트 문제를 완화하기 위해, 심층 신경망에서 사용했던 기법을 RNN에서 사용 가능하다.

- 좋은 가중치 초기화

- 빠른 Optimizer

- Drop out

<br />

하지만 **수렴하지 않는 활성화 함수**는 RNN을 불안정하게 만든다.

- 경사 하강법이 첫 번째 타입스텝에서 출력을 조금 증가시키는 방향으로 가중치를 업데이트한다고 가정해보면, **동일한 가중치가 모든 타임 스텝에서 사용**되므로 두 번째 타임 스텝의 출력도 조금 증가할 수 있다. (출력이 폭주)

- 수렴하지 않는 활성화 함수는 이를 막지 못하여, **하이퍼볼릭 탄젠트** 같은 **수렴하는(saturating) 활성화 함수를 사용한다.**

<br />

**Batch Normalization(배치 정규화)** 는 심층 feedforward 네트워크처럼 RNN과 효율적으로 사용할 수 없다.

- 타임 스텝 사이에 사용할 수 없고, 순환 층 사이에만 가능하다. 이는 엄청난 효과를 주지는 않는다.

- 그렇기에 RNN에서 잘 맞는 다른 종류의 정규화인, **Layer Normalization(층 정규화)** 를 이용한다.

<br />

**Layer Normalization(층 정규화)**

- 배치 차원에 대해 정규화 하는 대신 **features 차원에 대해 정규화**한다.

- 샘플에 독립적으로 타임스텝마다 종적으로 필요한 통계를 계산할 수 있다.

- 훈련 세트의 모든 샘플에 대한 feature 통계를 추정하기 위해 지수 이동 평균이 필요하지 않다. 

- 입력마다 하나의 스케일과 이동 파라미터를 학습한다.

- RNN에서 층 정규화는 보통 입력과 은닉 상태의 선형 조합 직후에 사용된다.

<br />

# Solution

<hr />

## Tackling short-term memory problem

> RNN을 거치면서 데이터가 변환되므로 일부 정보는 매 훈련 스텝 후 사라진다. 어느 정도 지나면, RNN의 상태는 사실상 첫 번째 입력의 흔적을 가지고 있지 않는다. 따라서 이를 해결하기 위해 Long Short-Term Memory(LSTM)을 사용한다.

## LSTM cells

LSTM은 훈련이 더 빨리 수렴되고, **데이터의 장기적인 종속성**을 감지한다.

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145703081-670c84fe-15dc-4037-b65c-88359aba4b04.png" width="600">
</div>

- 박스 내부를 보지 않는다면, 상태가 두 개의 벡터 h(t)와 c(t) (c는 셀을 의미) 로 나뉜다는 것을 빼고는 일반 셀과 같다.

- h(t)를 단기 상태, c(t)를 장기 상태라고 할 수 있다.

- 내부 구조

  - 장기 기억 c(t-1)은 네트워크를 왼쪽에서 오른쪽으로 관통하면서, **Forget gate**를 지나 일부 기억을 잃고, 그 후 덧셈 연산으로 새로운 기억 일부를 추가한다. ➡️ **Input gate**에서 선택한 기억을 추가

  - 만들어진 c(t)는 다른 추가 변환 없이 바로 출력으로 보내진다. ➡️ **타입스텝마다 일부 기억이 삭제되고 일부 기억이 추가된다.**

  - 또한 덧셈 연산 후 장기 상태가 복사되어 **tanh** 함수로 전달된다.

  - 그 후 이 결과는 **Output gate**에 의해 걸러진다. ➡️ 단기 상태 h(t) 생성

- **Forget gate**는 장기 상태의 어느 부분이 삭제되어야 하는지 제어한다.

- **Input gate**는 어느 부분이 장기 상태에 더해져야 하는지 제어한다.

- **Output gate**는 장기 상태의 어느 부분을 읽어서 이 타임스텝의 h(t)와 y(t)로 출력해야 하는지 제어한다.

**요약하자면, LSTM 셀은 중요한 입력을 인식하고(Input gate) 장기 상태에 저장하며, 필요한 기간 동안 이를 보존하여(Forget gate) 필요할 때마다 이를 추출하기 위해 학습한다.**

<br />

## GRU cells

**Gated Recurrent Unit (GRU) 셀은 LSTM 셀의 간소화 버전이다.**

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145703276-fc2e9556-5fb7-43a9-9347-60cb9aa085b7.png" width="500">
</div>

- **두 상태 벡터가 하나의 벡터 h(t)로 합쳐졌다.**

- **하나의 게이트 제어기 z(t)가 Forget 게이트와 Input 게이트 모두를 제어한다.**

  - 게이트 제어기가 1을 출력하면 Forget 게이트가 열리고 Input 게이트가 닫힌다.

  - 게이트 제어기가 0을 출력하면 Input 게이트가 열리고 Forget 게이트가 닫힌다.

  ➡️ 메모리가 저장될 때마다 저장될 위치가 먼저 삭제된다.

- **Output gate가 없다.**

  - 전체 상태 벡터가 매 타임 스텝마다 출력된다.

  - 이전 상태의 어느 부분이 main 층에 노출될지 제어하는 새로운 게이트 제어기 r(t)가 있다.

  <br />

LSTM과 GRU 셀은 RNN 성공의 주역이다. **단순한 RNN보다 훨씬 긴 시퀀스를 다루지만, 매우 제한적인 단기 메모리를 가졌다.** ➔ 100단계 이상과 같은 장기 패턴은 배우기 어렵다.

**이를 해결하는 한 가지 방법은 입력 시퀀스를 줄이는 것이다.**

<br /> 

## 1D convolutional layer

> `Conv1D`는 `Conv2D`의 개념을 1차원으로 옮긴 것이다. 컨볼루션 층이 1차원이고 이동하는 배열도 1차원이다. 1D 컨볼루션 층이 몇 개의 커널을 시퀀스 위를 슬라이딩하여 커널마다 1D feature maps을 출력한다.

- 각 커널은 매우 짧은 하나의 순차 패턴을 감지하도록 학습한다.

- 10개의 커널을 사용하면 이 층의 출력은 10개의 1차원 시퀀스로 구성된다. (= 10차원 시퀀스)

<div style="text-align:center">
  <img src="https://user-images.githubusercontent.com/33220404/145703509-e8e948b6-67db-4a02-85ea-6c286844440f.png" width="800">
</div>

- 노란색 레이어: 마스크(=필터, 커널)

- **이 마스크가 지나가면서 원래의 1차원 배열에 가중치를 각각 곱하여 새로운 층인 컨볼루션 층을 만든다.**

**모델이 중요하지 않은 건 버리고, 유용한 정보를 보존하도록 학습하여 컨볼루션 층으로 시퀀스 길이를 줄이면, 더 긴 패턴을 감지하는 데 도움이 된다.**

<br />

## 1D max pooling

MaxPooling1D 역시 마찬가지이다. 2차원 배열이 1차원으로 바뀌어 **정해진 구역 안에서 가장 큰 값을 다음 층으로 넘기고 나머지는 버린다.**

<br />
<br />

**Source:**

📖 핸즈온머신러닝, 2/E, 2020 (번역)

📖 기계학습, 오일석, 2017

📖 모두의 딥러닝, 2/E, 2020
